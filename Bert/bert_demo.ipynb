{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dict(text):\n",
    "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "    word_list = list(set(\" \".join(sentences).split()))\n",
    "    word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "    \n",
    "    for i,w in enumerate(word_list):\n",
    "        word_dict[w] = i+4\n",
    "    \n",
    "    number_dict=zip(word_dict.values(),word_dict.keys())\n",
    "    \n",
    "    vocab_size = len(word_dict)\n",
    "    \n",
    "    token_list=[[word_dict[word] for word in sentence.split()] for sentence in sentences]\n",
    "    \n",
    "    return sentences,word_dict,number_dict,token_list,vocab_size\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences,word_dict,number_dict,token_list,batch_size,max_pred,vocab_size,max_len):\n",
    "    \n",
    "    \"\"\"\n",
    "    max_pred:max tokens of prediction\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    \n",
    "    positive = negative = 0\n",
    "    \n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "    \n",
    "        tokens_a_index,token_b_index = randrange(len(sentences)),randrange(len(sentences))\n",
    "        tokens_a,tokens_b = token_list[tokens_a_index],token_list[token_b_index]\n",
    "        input_ids = [word_dict[\"[CLS]\"]]+tokens_a + [word_dict[\"[SEP]\"]] + tokens_b + [word_dict[\"[SEP]\"]]\n",
    "        segment_ids = [0]*(1+len(tokens_a)+1)+[1]*(len(tokens_b)+1)\n",
    "\n",
    "        #MASK LM\n",
    "\n",
    "        n_pred = min(max_pred,max(1,int(round(len(input_ids)*15)))) # 15% of tokens in one sentence to mask\n",
    "\n",
    "        can_masked_pos = [i for i,token in enumerate(input_ids) if token != word_dict['[CLS]' and token != word_dict['[SEP]']]]\n",
    "\n",
    "        shuffle(can_masked_pos)\n",
    "\n",
    "        masked_tokens,masked_pos = [],[]\n",
    "\n",
    "        for pos in can_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random()<0.8:\n",
    "                input_ids[pos] = word_dict[\"[MASK]\"]\n",
    "            elif 0.8<random()<0.9:\n",
    "                index = randint(0,vocab_size-1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[num_dict[index]] # replace\n",
    "\n",
    "\n",
    "        n_pad = maxlen-len(input_ids)\n",
    "        input_ids.extend([0]*n_pad)\n",
    "        segment_ids.extend([0]*n_pad)\n",
    "\n",
    "        if max_pred>n_pred:\n",
    "            n_pad = max_pred-n_pred\n",
    "            masked_tokens.extend([0]*n_pad)\n",
    "            masked_pos.extend([0]*n_pad)\n",
    "        if tokens_a_index +1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segement_ids,masked_tokens,masked_pos,True]) #isNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index+1!= tokens_b_index and negative<batch_size/2:\n",
    "            batch.append([input_ids,segment_ids,masked_tokens,masked_pos,False]) # NotNext\n",
    "            negative +=1\n",
    "    return batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q,seq_k):\n",
    "    \n",
    "    batch_size ,len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    \n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    \n",
    "    return pad_attn_mask.expand(batch_size,len_q,len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,model_d,max_len,n_segments):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_embed = nn.Embedding(vocab_size,model_d)\n",
    "        self.pos_embed = nn.Embedding(max_len,model_d)\n",
    "        self.seg_embed = nn.Embedding(n_segments,model_d)\n",
    "        self.norm = nn.LayerNorm(model_d)\n",
    "    \n",
    "    def forward(self,x,seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len,dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)\n",
    "        embedding = self.tok_embed(x)+self.pos_embed(pos)+self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention,self).__init__()\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q,K.transpose(-1,-2))/np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask,-1e9)\n",
    "        attn = self.sofmax(scores)\n",
    "        context = torch.matmul(attn,v)\n",
    "        return context,attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,model_d,d_k,n_heads):\n",
    "        super().__init__()\n",
    "        self.W_Q=nn.Linear(model_d,d_k*n_heads)\n",
    "        self.W_K=nn.Linear(model_d,d_k*n_heads)\n",
    "        self.W_V=nn.Linear(model_d,d_k*n_heads)\n",
    "        self.linear = nn.Linear(n_heads*d_k,model_d)\n",
    "        self.norm = nn.LayerNorm(model_d)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,Q,K,V,attn):\n",
    "        residual, batch_size = Q,Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "        \n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        output = self.linear(context)\n",
    "        return self.norm(output+residual),attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,model_d,d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(model_d,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,model_d)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,model_d,d_k,n_heads,d_ff):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_self_attn = MultiHeadAttention(model_d,d_k,n_heads)\n",
    "        self.pos_ffn = PowriseFeedForward(model_d,d_ff)\n",
    "    \n",
    "    def forward(self,enc_inputs,enc_self_attn_mask):\n",
    "        enc_outputs,attn = self.enc_self_attn(enc_inputs,enc_inputs,enc_inputs,enc_self_attn_mask)\n",
    "        enc_ouputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs,attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.Tensor([[[1,2,3],[4,5,6]],\n",
    "             [[7,8,9],[10,11,12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1=torch.Tensor([[1,2,3],[4,5,6]])\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "         [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.],\n",
       "         [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.unsqueeze(-1).expand(-1,-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [2.],\n",
       "         [3.]],\n",
       "\n",
       "        [[4.],\n",
       "         [5.],\n",
       "         [6.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
